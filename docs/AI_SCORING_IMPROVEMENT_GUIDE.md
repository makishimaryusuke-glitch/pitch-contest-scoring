# AI採点システムの精度・一貫性向上ガイド

## 📊 現状の課題

現在のAI採点システムは基本的なプロンプトエンジニアリングで実装されていますが、以下の課題があります：

1. **評価基準が抽象的** - 「高校生らしい柔軟な発想」などの主観的な表現
2. **スコアリング基準が不明確** - 何点がどのレベルかが明確でない
3. **一貫性の検証が不足** - 同じ資料を複数回採点した場合のばらつきが不明
4. **学習データの不足** - 過去の採点データや優秀作品の例がない

## 🎯 精度・一貫性向上のために必要な情報

### 1. **評価基準の具体化（最重要）**

#### 現在の問題点
- プロンプトが抽象的で、AIが判断に迷う可能性がある
- スコアリングの基準が不明確

#### 必要な情報
```
各評価項目について、以下の情報を準備：

1. スコアリングルーブリック
   - 10点: どのような状態か（具体例）
   - 8-9点: どのような状態か
   - 6-7点: どのような状態か
   - 4-5点: どのような状態か
   - 0-3点: どのような状態か

2. チェックポイントリスト
   - 評価する際に確認すべき具体的な要素
   - 例：「着眼点の独創性」の場合
     - 既存の研究や手法を参考にしているか
     - 独自の視点やアプローチがあるか
     - 高校生らしい柔軟な発想があるか
     - など

3. 評価例（Few-shot Learning用）
   - 高得点（8-10点）の具体例
   - 中得点（5-7点）の具体例
   - 低得点（0-4点）の具体例
```

### 2. **過去の採点データの蓄積**

#### 必要なデータ
- **審査員による人的採点データ**
  - 過去のコンテストでの採点結果
  - 各評価項目のスコアと評価理由
  - 審査員間の採点のばらつき

- **優秀作品の例**
  - 高得点を獲得した作品の提出資料
  - 各評価項目で高得点を獲得した作品の特徴
  - 低得点の作品との比較

- **スコア分布データ**
  - 過去の採点結果の分布
  - 平均点、標準偏差
  - 評価項目ごとの傾向

### 3. **検証・校正データセット**

#### 推奨されるデータセット
```
1. 基準データセット（10-20作品）
   - 複数の審査員が採点した作品
   - 審査員間の合意度が高い作品
   - 各スコア帯（高・中・低）から均等に選択

2. 検証用データセット（5-10作品）
   - AI採点と人的採点を比較するための作品
   - 定期的に再採点して一貫性を確認
```

### 4. **ドメイン知識の追加**

#### SPLYZAMotionに関する情報
- SPLYZAMotionの特徴や機能
- データの活用方法の例
- よくある分析パターン
- 期待される活用事例

#### スポーツ科学・データ分析の知識
- スポーツデータ分析の基本
- 統計的な検証方法
- 科学的な探究の進め方

## 🔧 実装すべき改善策

### 1. **プロンプトの改善**

#### 現在のプロンプトの問題点
```python
# 現在：抽象的
"評価基準：既存の枠にとらわれない、高校生らしい柔軟な発想やユニークな視点があるか。"
```

#### 改善案：具体的なルーブリックを含める
```python
"""
評価基準：着眼点の独創性（0-10点）

【スコアリング基準】
- 10点: 既存の研究や手法を超えた、非常に独創的で革新的な視点がある。高校生らしい柔軟な発想が際立っている。
- 8-9点: 既存の手法を参考にしつつも、独自の視点やアプローチが明確にある。
- 6-7点: 既存の手法を参考にしているが、一部に独自の視点が見られる。
- 4-5点: 主に既存の手法を参考にしており、独自性が少ない。
- 0-3点: 既存の手法の模倣に留まり、独自性が見られない。

【評価のポイント】
1. 既存の研究や手法を参考にしているか
2. 独自の視点やアプローチがあるか
3. 高校生らしい柔軟な発想があるか
4. ユニークな視点が明確に示されているか

【評価例】
- 高得点例: 「SPLYZAMotionのデータを活用して、従来の練習方法とは異なる、個人の特性に合わせた練習メニューを提案した」
- 中得点例: 「SPLYZAMotionのデータを分析し、一般的な練習方法を確認した」
- 低得点例: 「既存の研究をそのまま引用し、独自の視点が見られない」
"""
```

### 2. **Few-shot Learningの実装**

#### プロンプトに具体例を追加
```python
"""
以下の提出資料を読み、評価基準に基づいて0-10点で採点してください。

【評価基準】
（上記のルーブリック）

【評価例】
例1: 「SPLYZAMotionのデータを活用して...」（評価: 9点、理由: 独自の視点が明確）
例2: 「既存の研究を参考に...」（評価: 6点、理由: 独自性が少ない）

【提出資料】
{content}

採点結果は以下のJSON形式で返してください：
{
    "score": 0-10の整数,
    "reason": "採点理由を日本語で200文字程度で説明"
}
"""
```

### 3. **一貫性の確保**

#### 複数回採点の平均化
```python
def evaluate_criterion_with_consistency(content: str, criterion_id: int, n_times: int = 3) -> Dict[str, any]:
    """複数回採点して平均を取る（一貫性向上）"""
    scores = []
    reasons = []
    
    for _ in range(n_times):
        result = evaluate_criterion(content, criterion_id)
        scores.append(result['score'])
        reasons.append(result['reason'])
    
    avg_score = round(sum(scores) / len(scores))
    
    return {
        "score": avg_score,
        "reason": f"平均スコア: {avg_score}点（{n_times}回採点の平均）",
        "scores": scores,
        "reasons": reasons
    }
```

#### 温度設定の最適化
```python
# 現在: temperature=0.3（低めで一貫性重視）
# 推奨: 0.1-0.2（さらに低くして一貫性を高める）
temperature=0.1  # より一貫性の高い採点
```

### 4. **検証機能の追加**

#### 人的採点との比較
```python
def compare_with_human_scoring(ai_score: int, human_scores: List[int]) -> Dict:
    """AI採点と人的採点を比較"""
    avg_human_score = sum(human_scores) / len(human_scores)
    difference = abs(ai_score - avg_human_score)
    
    return {
        "ai_score": ai_score,
        "human_avg": avg_human_score,
        "difference": difference,
        "correlation": calculate_correlation(ai_score, human_scores)
    }
```

## 📋 実装優先順位

### 高優先度（すぐに実装すべき）
1. ✅ **評価基準の具体化** - ルーブリックの作成
2. ✅ **プロンプトの改善** - 具体的な評価基準を追加
3. ✅ **温度設定の最適化** - 0.1-0.2に変更

### 中優先度（データ収集後）
4. ⏳ **Few-shot Learning** - 評価例をプロンプトに追加
5. ⏳ **複数回採点の平均化** - 一貫性向上
6. ⏳ **検証機能の追加** - 人的採点との比較

### 低優先度（長期的な改善）
7. 🔮 **ファインチューニング** - 過去の採点データでモデルを学習
8. 🔮 **アンサンブル手法** - 複数のAIモデルで採点して平均
9. 🔮 **自動校正システム** - 異常値の検出と再採点

## 🎓 学習データの準備方法

### ステップ1: 基準データセットの作成
1. 過去のコンテストから10-20作品を選択
2. 複数の審査員（3-5名）に採点してもらう
3. 審査員間の合意度が高い作品を選定
4. 各スコア帯から均等に選択

### ステップ2: 評価基準の明確化
1. 審査員と協力してルーブリックを作成
2. 各評価項目のスコアリング基準を明確化
3. 評価例（高・中・低）を収集

### ステップ3: 検証データセットの作成
1. 基準データセットとは別の5-10作品を選択
2. AI採点と人的採点を比較
3. 差異が大きい場合はプロンプトを改善

## 📊 評価指標

### 精度の評価
- **相関係数**: AI採点と人的採点の相関（目標: 0.8以上）
- **平均絶対誤差**: AI採点と人的採点の差の平均（目標: 1点以内）
- **一致率**: 完全一致する割合（目標: 60%以上）

### 一貫性の評価
- **再現性**: 同じ資料を複数回採点した場合のばらつき（目標: 標準偏差1点以内）
- **評価者間信頼性**: 複数のAIモデルで採点した場合の一致度

## 🔍 継続的な改善

### 定期的な検証
1. **月次レビュー**: 採点結果の分布を確認
2. **四半期レビュー**: 人的採点との比較
3. **年次レビュー**: プロンプトの見直しと改善

### フィードバックループ
1. 審査員からのフィードバックを収集
2. 異常値や問題のある採点を特定
3. プロンプトを改善して再検証

## 📝 まとめ

AI採点システムの精度と一貫性を向上させるには：

1. **評価基準の具体化**が最も重要
2. **過去の採点データ**を蓄積して学習
3. **Few-shot Learning**で具体例を提供
4. **検証データセット**で定期的に評価
5. **継続的な改善**で精度を向上

まずは**評価基準の具体化**と**プロンプトの改善**から始めることをお勧めします。

